{{- if .Values.config.typha.enabled }}
---
# This manifest creates a Deployment of Typha to back the above service.
apiVersion: apps/v1
kind: Deployment
metadata:
  # We changed the name of the default deployment name from "calico-typha" to "calico-typha-deploy" because
  # Gardener deployed calico-typha with 0 replicas. Now, we don't specify `.spec.replicas` anymore (because we
  # use the typha-HPA for auto-scaling), hence, for existing clusters the replicas will never get `1` (because
  # the typha-HPA is a new pod that cannot start without typha to run).
  name: calico-typha-deploy
  namespace: kube-system
  annotations:
    cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
    {{- if not .Values.autoscaling.staticRequests }}
    resources.gardener.cloud/preserve-resources: 'true'
    {{- end }}
  labels:
    k8s-app: calico-typha
    gardener.cloud/role: system-component
spec:
  revisionHistoryLimit: 5
  strategy:
    # Ensure that typha always stays available
    type: RollingUpdate
    rollingUpdate:
      # We would want maxUnavailable=0, but for one node cluster this would block updates
      # as the port would already be occupied due to usage of host network
      maxUnavailable: 1
      # 100% surge allows a complete up-level set of typha instances to start and become ready,
      # which in turn allows all the back-level typha instances to start shutting down. This
      # means that connections tend to bounce directly from a back-level instance to an up-level
      # instance.
      maxSurge: 100%
  selector:
    matchLabels:
      k8s-app: calico-typha
  # Number of Typha replicas.  To enable Typha, set this to a non-zero value *and* set the
  # typha_service_name variable in the calico-config ConfigMap above.
  #
  # We recommend using Typha if you have more than 50 nodes.  Above 100 nodes it is essential
  # (when using the Kubernetes datastore).  Use one replica for every 100-200 nodes.  In
  # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade.
  template:
    metadata:
      labels:
        networking.gardener.cloud/to-public-networks: allowed
        networking.gardener.cloud/to-apiserver: allowed
        networking.gardener.cloud/to-dns: allowed
        origin: gardener
        gardener.cloud/role: system-component
        k8s-app: calico-typha
    spec:
      tolerations:
      # Make sure typha gets scheduled on all nodes.
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      # Since Calico can't network a pod until Typha is up, we need to run Typha itself
      # as a host-networked pod.
      hostNetwork: true
      # Typha supports graceful shut down, disconnecting clients slowly during the grace period.
      # The TYPHA_SHUTDOWNTIMEOUTSECS env var should be kept in sync with this value.
      terminationGracePeriodSeconds: 300
      priorityClassName: system-cluster-critical
      serviceAccountName: calico-typha
      securityContext:
        runAsUser: 65534
        fsGroup: 65534
        supplementalGroups:
        - 1
        seccompProfile:
          type: RuntimeDefault
      containers:
      - image: {{ index .Values.images "calico-typha" }}
        imagePullPolicy: IfNotPresent
        name: calico-typha
        ports:
        - containerPort: 5473
          name: calico-typha
          protocol: TCP
        {{- if .Values.config.monitoring.enabled }}
        - containerPort: {{ .Values.config.monitoring.typhaMetricsPort }}
          name: metrics
          protocol: TCP
        {{-  end }}
        env:
          {{- if eq .Values.config.ipam.type "host-local"}}
          - name: USE_POD_CIDR
            value: "true"
          {{- end }}
          {{- if .Values.config.monitoring.enabled }}
          - name: TYPHA_PROMETHEUSMETRICSENABLED
            value: "{{ .Values.config.monitoring.enabled }}"
          - name: TYPHA_PROMETHEUSMETRICSPORT
            value: "{{ .Values.config.monitoring.typhaMetricsPort }}"
          {{-  end }}
          # Enable "info" logging by default.  Can be set to "debug" to increase verbosity.
          - name: TYPHA_LOGSEVERITYSCREEN
            value: "info"
          # Disable logging to file and syslog since those don't make sense in Kubernetes.
          - name: TYPHA_LOGFILEPATH
            value: "none"
          - name: TYPHA_LOGSEVERITYSYS
            value: "none"
          # Monitor the Kubernetes API to find the number of running instances and rebalance
          # connections.
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: "kubernetes"
          - name: TYPHA_DATASTORETYPE
            value: "kubernetes"
          - name: TYPHA_HEALTHENABLED
            value: "true"
          # Set this to the same value as terminationGracePeriodSeconds; it tells Typha how much time
          # it has to shut down.
          - name: TYPHA_SHUTDOWNTIMEOUTSECS
            value: "300"
          # Uncomment these lines to enable prometheus metrics. Since Typha is host-networked,
          # this opens a port on the host, which may need to be secured.
          #- name: TYPHA_PROMETHEUSMETRICSENABLED
          #  value: "true"
          #- name: TYPHA_PROMETHEUSMETRICSPORT
          #  value: "9093"
        resources:
          requests:
            cpu: {{ if and .Values.autoscaling.staticRequests .Values.autoscaling.resourceRequests.typha.cpu }}{{ .Values.autoscaling.resourceRequests.typha.cpu }}{{ else }}200m{{ end }}
            memory: {{ if and .Values.autoscaling.staticRequests .Values.autoscaling.resourceRequests.typha.memory }}{{ .Values.autoscaling.resourceRequests.typha.memory }}{{ else }}100Mi{{ end }}
          limits:
            memory: 4000Mi
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9098
            host: localhost
          periodSeconds: 30
          initialDelaySeconds: 30
          timeoutSeconds: 10
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9098
            host: localhost
          periodSeconds: 10
          timeoutSeconds: 10
{{- end }}
